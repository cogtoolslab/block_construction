{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo as pm\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "import itertools\n",
    "from itertools import groupby\n",
    "\n",
    "## plotting\n",
    "import  matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## connect to db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) establish tunnel to lab server: \n",
    "- Run this in terminal: `ssh -fNL 27017:127.0.0.1:27017 USER@cogtoolslab.org`\n",
    "OR\n",
    "\n",
    "- Do this once: Add this to your .bashrc or (.zshrc), then run `source .bashrc`:\n",
    "`alias tunnel_cogtoolslab=\"ssh -fNL 27017:127.0.0.1:27017 USER@cogtoolslab.org\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) establish connection to target db and collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set vars \n",
    "auth = pd.read_csv('auth.txt', header = None) # this auth.txt file contains the password for the sketchloop user\n",
    "pswd = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'cogtoolslab.org' ## experiment server ip address\n",
    "\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1')\n",
    "db = conn['stimuli']\n",
    "coll = db['block-construction-silhouette']\n",
    "\n",
    "# raw stimulus data\n",
    "path_to_interesting_structures = 'interesting_structures'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print list of database names \n",
    "# then list of collection names within stimuli db\n",
    "try:\n",
    "    print('dbnames:')\n",
    "    print(conn.list_database_names())\n",
    "    print('colnames:')\n",
    "    print(conn['stimuli'].list_collection_names())\n",
    "except:    ## if running in python2 notebook\n",
    "    print('dbnames:')\n",
    "    print(conn.database_names())\n",
    "    print('colnames:')\n",
    "    print(conn['stimuli'].collection_names())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the stimulus dictionary\n",
    "- In the design of silhouette-study-1, there are two conditions: ['physical', 'mental'], which will be manipulated within-participant.\n",
    "- At the beginning of each experimental session, all of the trials to construct a single session are fetched from the database. \n",
    "- Each trial dictionary needs to contain the following attributes: \n",
    "    - `target`: List of blocks generated elsewhere in this `stimuli/` dir\n",
    "    - `condition`: \"external\" can be a placeholder\n",
    "    - `games`: Empty games list that will be populated by `getstims` inside `store.js` as games fetch this type of game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    return [item for sublist in x for item in sublist]\n",
    "\n",
    "def get_uniq_orders(X):\n",
    "    return [list(x) for x in set(tuple(x) for x in X)]\n",
    "\n",
    "def get_longest_run(a):\n",
    "    '''\n",
    "    given a list a, return the length of the longest consecutive streak\n",
    "    '''\n",
    "    lst = []\n",
    "    for n,c in groupby(a):\n",
    "        num,count = n,sum(1 for i in c)\n",
    "        lst.append((num,count))\n",
    "    maxx = max([y for x,y in lst])\n",
    "    return maxx\n",
    "\n",
    "\n",
    "def get_all_good_seqs(file_list, conditions = ['a','b'], miniblock_size = 4):\n",
    "    \n",
    "    '''\n",
    "    file_list = list of uniq stims \n",
    "    miniblock_size = length of epoch that we \n",
    "    '''    \n",
    "    num_stims = len(file_list)\n",
    "    assert num_stims%miniblock_size==0        \n",
    "    miniblock_template = np.tile(conditions, int(miniblock_size/len(conditions))) ## ['mental','physical','mental','physical']        \n",
    "    all_permutes = list(itertools.permutations(miniblock_template))\n",
    "    uniq_orders = list(dict.fromkeys(all_permutes))\n",
    "    num_miniblocks = int(num_stims/4)\n",
    "\n",
    "    ## randomly sample miniblocks of size 4, concatenate, then measure center of mass of each condition\n",
    "    com1 = [] ## center of mass of condition 1\n",
    "    com2 = [] ## center of mass of condition 2\n",
    "    cond_seq = []\n",
    "    numIters = 5000\n",
    "    for thisIter in np.arange(numIters):\n",
    "        b = np.random.RandomState(thisIter).choice(np.arange(len(uniq_orders)),size=num_miniblocks,replace=True)\n",
    "        _cond_seq = flatten([uniq_orders[_b] for _b in b])\n",
    "        cond_seq.append(_cond_seq)\n",
    "        com1.append(np.where(np.array(_cond_seq)=='mental')[0].mean())\n",
    "        com2.append(np.where(np.array(_cond_seq)=='physical')[0].mean())\n",
    "    com1, com2 = map(np.array,[com1,com2])\n",
    "\n",
    "    ## CRITERION 1: mean position of each condition equated\n",
    "    ## get unique orderings from randomly sampled sequences where center of mass is equal between conditions\n",
    "    not_diff_inds = np.where(com1-com2==0)[0]\n",
    "    x = get_uniq_orders([cond_seq[_i] for _i in not_diff_inds])\n",
    "\n",
    "    ## CRITERION 2: no streaks > streak_thresh (e.g., 3)\n",
    "    streak_thresh = 3\n",
    "    nostreak_inds = np.where(np.array(list(map(get_longest_run,x))) < streak_thresh)[0]\n",
    "\n",
    "    ## get list of good sequences that survive: (1) mean position of each condition equated; (2) no streaks > length of 3\n",
    "    good_seqs = [x[i] for i in nostreak_inds]\n",
    "\n",
    "    return good_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build list of trial lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in list of structures\n",
    "file_list = os.listdir(path_to_interesting_structures)\n",
    "print('There are {} interesting structures to load.'.format(len(file_list)))\n",
    "\n",
    "## get list of good sequences\n",
    "good_seqs = get_all_good_seqs(file_list, conditions = ['mental','physical'], miniblock_size = 4)\n",
    "print('There are {} \"good\" sequences that satisfy our criteria.'.format(len(good_seqs)))\n",
    "\n",
    "## loop through list of good sequences, and build list of versions, each containing a trial list \n",
    "Meta = [] ## initialize list of all trial lists\n",
    "for version_ind, seq in enumerate(good_seqs):    \n",
    "    trial_list = []\n",
    "    for i,f in enumerate(file_list):\n",
    "        stim_list = pd.read_json(os.path.join(path_to_interesting_structures,f),orient='records') ## stim list\n",
    "        this_targetBlocks = list(stim_list['blocks'].values)\n",
    "        this_targetName = f.split('.')[0]\n",
    "        this_trial = {'targetBlocks':this_targetBlocks,\n",
    "                      'targetName': this_targetName, \n",
    "                      'trialNum' : i,\n",
    "                      'condition': seq[i],\n",
    "                      'versionInd': version_ind}\n",
    "        trial_list.append(this_trial) ## this is not particularly elegant -- I AM NOT PROUD, @jefan.\n",
    "    \n",
    "    ## convert to dataframe\n",
    "    trial_df = pd.DataFrame(trial_list)\n",
    "    stimList = trial_df.to_dict(orient='records')\n",
    "    \n",
    "    ## bundle all of the stims into stimDict, under the 'meta' key, and also add 'games' list \n",
    "    ## (to keep track of which games) and numTrials\n",
    "    stimDict = {}\n",
    "    stimDict['meta'] = stimList\n",
    "    stimDict['games'] = []\n",
    "    stimDict['numTrials'] = len(stimList)\n",
    "    stimDict['experimentName'] = 'silhouette-testing1' ## way of keeping track of different stimulus sets\n",
    "    stimDict['versionInd'] = version_ind\n",
    "    ## append stimdict to Meta\n",
    "    Meta.append(stimDict) \n",
    "\n",
    "print('Created Metadata.')\n",
    "\n",
    "## save out Meta as json\n",
    "MetaDF = pd.DataFrame(Meta)\n",
    "MetaDF.to_json('stimDict.json',orient='records')\n",
    "\n",
    "## Load metadata back in as a list of dictionaries\n",
    "import json\n",
    "J = json.loads(open('stimDict.json',mode='r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now, iterate through each version and insert into mongo\n",
    "## loop through list of records and insert each into collection\n",
    "reallyRun = 0\n",
    "if reallyRun:\n",
    "    for (i,j) in enumerate(J):        \n",
    "        if 'meta' in j.keys(): ## if 'meta' is accessible from the top of stimDict, implies that only one version of trial sequence\n",
    "            coll.insert_one(j)\n",
    "            print('Inserted {} version of stimDict.'.format(j['versionInd']))\n",
    "            clear_output(wait=True)\n",
    "        else:\n",
    "            print('Oops, the stimDict is missing the \"meta\" key.')\n",
    "else:\n",
    "    print('Did not insert any new data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect and validate collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} records in this collection.'.format(coll.estimated_document_count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Now let's look at one of the records\n",
    "coll.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: run sanity checks to make sure that every target structure appears exactly once per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
